{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPr2QhJ3ybJI12rtedzbKEg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JTStephens18/Neural-Fields-As-Learnable-Kernels-Paper-Implementation/blob/main/Neural_Fields_as_Learnable_Kernels_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "8LAifgaOGfh8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aebi2KozEWtD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from scipy.interpolate import interpn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)\n",
        "!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "! pip install torchinfo"
      ],
      "metadata": {
        "id": "bg9vwOumGjCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "import sys\n",
        "# import torch\n",
        "need_pytorch3d=False\n",
        "try:\n",
        "    import pytorch3d\n",
        "except ModuleNotFoundError:\n",
        "    need_pytorch3d=True\n",
        "if need_pytorch3d:\n",
        "    if torch.__version__.startswith(\"2.1.\") and sys.platform.startswith(\"linux\"):\n",
        "        # We try to install PyTorch3D via a released wheel.\n",
        "        pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "        version_str=\"\".join([\n",
        "            f\"py3{sys.version_info.minor}_cu\",\n",
        "            torch.version.cuda.replace(\".\",\"\"),\n",
        "            f\"_pyt{pyt_version_str}\"\n",
        "        ])\n",
        "        !pip install fvcore iopath\n",
        "        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
        "    else:\n",
        "        # We try to install PyTorch3D from source.\n",
        "        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"
      ],
      "metadata": {
        "id": "yfsUBZ5hGk-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_scatter import scatter_mean, scatter_max\n",
        "from torchinfo import summary\n",
        "from pytorch3d.ops import sample_farthest_points\n",
        "from pytorch3d.loss import chamfer_distance"
      ],
      "metadata": {
        "id": "S0Dx7LNPGrzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/save/path/augmentedSurfacePoints.zip\"\n",
        "!unzip \"/save/path/occupancyPoints_1.zip\"\n",
        "!unzip \"/save/path/sampledSurfacePoints.zip\""
      ],
      "metadata": {
        "id": "NXQdPbkjK6-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "ASvzI-CZGuGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Copyright (C) 2012 Daniel Maturana\n",
        "#  This file is part of binvox-rw-py.\n",
        "#\n",
        "#  binvox-rw-py is free software: you can redistribute it and/or modify\n",
        "#  it under the terms of the GNU General Public License as published by\n",
        "#  the Free Software Foundation, either version 3 of the License, or\n",
        "#  (at your option) any later version.\n",
        "#\n",
        "#  binvox-rw-py is distributed in the hope that it will be useful,\n",
        "#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "#  GNU General Public License for more details.\n",
        "#\n",
        "#  You should have received a copy of the GNU General Public License\n",
        "#  along with binvox-rw-py. If not, see <http://www.gnu.org/licenses/>.\n",
        "#\n",
        "\n",
        "\"\"\"\n",
        "Binvox to Numpy and back.\n",
        "\n",
        "\n",
        ">>> import numpy as np\n",
        ">>> import binvox_rw\n",
        ">>> with open('chair.binvox', 'rb') as f:\n",
        "...     m1 = binvox_rw.read_as_3d_array(f)\n",
        "...\n",
        ">>> m1.dims\n",
        "[32, 32, 32]\n",
        ">>> m1.scale\n",
        "41.133000000000003\n",
        ">>> m1.translate\n",
        "[0.0, 0.0, 0.0]\n",
        ">>> with open('chair_out.binvox', 'wb') as f:\n",
        "...     m1.write(f)\n",
        "...\n",
        ">>> with open('chair_out.binvox', 'rb') as f:\n",
        "...     m2 = binvox_rw.read_as_3d_array(f)\n",
        "...\n",
        ">>> m1.dims==m2.dims\n",
        "True\n",
        ">>> m1.scale==m2.scale\n",
        "True\n",
        ">>> m1.translate==m2.translate\n",
        "True\n",
        ">>> np.all(m1.data==m2.data)\n",
        "True\n",
        "\n",
        ">>> with open('chair.binvox', 'rb') as f:\n",
        "...     md = binvox_rw.read_as_3d_array(f)\n",
        "...\n",
        ">>> with open('chair.binvox', 'rb') as f:\n",
        "...     ms = binvox_rw.read_as_coord_array(f)\n",
        "...\n",
        ">>> data_ds = binvox_rw.dense_to_sparse(md.data)\n",
        ">>> data_sd = binvox_rw.sparse_to_dense(ms.data, 32)\n",
        ">>> np.all(data_sd==md.data)\n",
        "True\n",
        ">>> # the ordering of elements returned by numpy.nonzero changes with axis\n",
        ">>> # ordering, so to compare for equality we first lexically sort the voxels.\n",
        ">>> np.all(ms.data[:, np.lexsort(ms.data)] == data_ds[:, np.lexsort(data_ds)])\n",
        "True\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Voxels(object):\n",
        "    \"\"\" Holds a binvox model.\n",
        "    data is either a three-dimensional numpy boolean array (dense representation)\n",
        "    or a two-dimensional numpy float array (coordinate representation).\n",
        "\n",
        "    dims, translate and scale are the model metadata.\n",
        "\n",
        "    dims are the voxel dimensions, e.g. [32, 32, 32] for a 32x32x32 model.\n",
        "\n",
        "    scale and translate relate the voxels to the original model coordinates.\n",
        "\n",
        "    To translate voxel coordinates i, j, k to original coordinates x, y, z:\n",
        "\n",
        "    x_n = (i+.5)/dims[0]\n",
        "    y_n = (j+.5)/dims[1]\n",
        "    z_n = (k+.5)/dims[2]\n",
        "    x = scale*x_n + translate[0]\n",
        "    y = scale*y_n + translate[1]\n",
        "    z = scale*z_n + translate[2]\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, dims, translate, scale, axis_order):\n",
        "        self.data = data\n",
        "        self.dims = dims\n",
        "        self.translate = translate\n",
        "        self.scale = scale\n",
        "        assert (axis_order in ('xzy', 'xyz'))\n",
        "        self.axis_order = axis_order\n",
        "\n",
        "    def clone(self):\n",
        "        data = self.data.copy()\n",
        "        dims = self.dims[:]\n",
        "        translate = self.translate[:]\n",
        "        return Voxels(data, dims, translate, self.scale, self.axis_order)\n",
        "\n",
        "    def write(self, fp):\n",
        "        write(self, fp)\n",
        "\n",
        "def read_header(fp):\n",
        "    \"\"\" Read binvox header. Mostly meant for internal use.\n",
        "    \"\"\"\n",
        "    line = fp.readline().strip()\n",
        "    if not line.startswith(b'#binvox'):\n",
        "        raise IOError('Not a binvox file')\n",
        "    dims = list(map(int, fp.readline().strip().split(b' ')[1:]))\n",
        "    translate = list(map(float, fp.readline().strip().split(b' ')[1:]))\n",
        "    scale = list(map(float, fp.readline().strip().split(b' ')[1:]))[0]\n",
        "    line = fp.readline()\n",
        "    return dims, translate, scale\n",
        "\n",
        "def read_as_3d_array(fp, fix_coords=True):\n",
        "    \"\"\" Read binary binvox format as array.\n",
        "\n",
        "    Returns the model with accompanying metadata.\n",
        "\n",
        "    Voxels are stored in a three-dimensional numpy array, which is simple and\n",
        "    direct, but may use a lot of memory for large models. (Storage requirements\n",
        "    are 8*(d^3) bytes, where d is the dimensions of the binvox model. Numpy\n",
        "    boolean arrays use a byte per element).\n",
        "\n",
        "    Doesn't do any checks on input except for the '#binvox' line.\n",
        "    \"\"\"\n",
        "    dims, translate, scale = read_header(fp)\n",
        "    raw_data = np.frombuffer(fp.read(), dtype=np.uint8)\n",
        "    # if just using reshape() on the raw data:\n",
        "    # indexing the array as array[i,j,k], the indices map into the\n",
        "    # coords as:\n",
        "    # i -> x\n",
        "    # j -> z\n",
        "    # k -> y\n",
        "    # if fix_coords is true, then data is rearranged so that\n",
        "    # mapping is\n",
        "    # i -> x\n",
        "    # j -> y\n",
        "    # k -> z\n",
        "    values, counts = raw_data[::2], raw_data[1::2]\n",
        "    data = np.repeat(values, counts).astype(bool)\n",
        "    data = data.reshape(dims)\n",
        "    if fix_coords:\n",
        "        # xzy to xyz TODO the right thing\n",
        "        data = np.transpose(data, (0, 2, 1))\n",
        "        axis_order = 'xyz'\n",
        "    else:\n",
        "        axis_order = 'xzy'\n",
        "    return Voxels(data, dims, translate, scale, axis_order)\n",
        "\n",
        "def read_as_coord_array(fp, fix_coords=True):\n",
        "    \"\"\" Read binary binvox format as coordinates.\n",
        "\n",
        "    Returns binvox model with voxels in a \"coordinate\" representation, i.e.  an\n",
        "    3 x N array where N is the number of nonzero voxels. Each column\n",
        "    corresponds to a nonzero voxel and the 3 rows are the (x, z, y) coordinates\n",
        "    of the voxel.  (The odd ordering is due to the way binvox format lays out\n",
        "    data).  Note that coordinates refer to the binvox voxels, without any\n",
        "    scaling or translation.\n",
        "\n",
        "    Use this to save memory if your model is very sparse (mostly empty).\n",
        "\n",
        "    Doesn't do any checks on input except for the '#binvox' line.\n",
        "    \"\"\"\n",
        "    dims, translate, scale = read_header(fp)\n",
        "    raw_data = np.frombuffer(fp.read(), dtype=np.uint8)\n",
        "\n",
        "    values, counts = raw_data[::2], raw_data[1::2]\n",
        "\n",
        "    sz = np.prod(dims)\n",
        "    index, end_index = 0, 0\n",
        "    end_indices = np.cumsum(counts)\n",
        "    indices = np.concatenate(([0], end_indices[:-1])).astype(end_indices.dtype)\n",
        "\n",
        "    values = values.astype(bool)\n",
        "    indices = indices[values]\n",
        "    end_indices = end_indices[values]\n",
        "\n",
        "    nz_voxels = []\n",
        "    for index, end_index in zip(indices, end_indices):\n",
        "        nz_voxels.extend(range(index, end_index))\n",
        "    nz_voxels = np.array(nz_voxels)\n",
        "    # TODO are these dims correct?\n",
        "    # according to docs,\n",
        "    # index = x * wxh + z * width + y; // wxh = width * height = d * d\n",
        "\n",
        "    x = nz_voxels / (dims[0]*dims[1])\n",
        "    zwpy = nz_voxels % (dims[0]*dims[1]) # z*w + y\n",
        "    z = zwpy / dims[0]\n",
        "    y = zwpy % dims[0]\n",
        "    if fix_coords:\n",
        "        data = np.vstack((x, y, z))\n",
        "        axis_order = 'xyz'\n",
        "    else:\n",
        "        data = np.vstack((x, z, y))\n",
        "        axis_order = 'xzy'\n",
        "\n",
        "    #return Voxels(data, dims, translate, scale, axis_order)\n",
        "    return Voxels(np.ascontiguousarray(data), dims, translate, scale, axis_order)\n",
        "\n",
        "def dense_to_sparse(voxel_data, dtype=int):\n",
        "    \"\"\" From dense representation to sparse (coordinate) representation.\n",
        "    No coordinate reordering.\n",
        "    \"\"\"\n",
        "    if voxel_data.ndim!=3:\n",
        "        raise ValueError('voxel_data is wrong shape; should be 3D array.')\n",
        "    return np.asarray(np.nonzero(voxel_data), dtype)\n",
        "\n",
        "def sparse_to_dense(voxel_data, dims, dtype=bool):\n",
        "    if voxel_data.ndim!=2 or voxel_data.shape[0]!=3:\n",
        "        raise ValueError('voxel_data is wrong shape; should be 3xN array.')\n",
        "    if np.isscalar(dims):\n",
        "        dims = [dims]*3\n",
        "    dims = np.atleast_2d(dims).T\n",
        "    # truncate to integers\n",
        "    xyz = voxel_data.astype(np.int)\n",
        "    # discard voxels that fall outside dims\n",
        "    valid_ix = ~np.any((xyz < 0) | (xyz >= dims), 0)\n",
        "    xyz = xyz[:,valid_ix]\n",
        "    out = np.zeros(dims.flatten(), dtype=dtype)\n",
        "    out[tuple(xyz)] = True\n",
        "    return out\n",
        "\n",
        "#def get_linear_index(x, y, z, dims):\n",
        "    #\"\"\" Assuming xzy order. (y increasing fastest.\n",
        "    #TODO ensure this is right when dims are not all same\n",
        "    #\"\"\"\n",
        "    #return x*(dims[1]*dims[2]) + z*dims[1] + y\n",
        "\n",
        "def write(voxel_model, fp):\n",
        "    \"\"\" Write binary binvox format.\n",
        "\n",
        "    Note that when saving a model in sparse (coordinate) format, it is first\n",
        "    converted to dense format.\n",
        "\n",
        "    Doesn't check if the model is 'sane'.\n",
        "\n",
        "    \"\"\"\n",
        "    if voxel_model.data.ndim==2:\n",
        "        # TODO avoid conversion to dense\n",
        "        dense_voxel_data = sparse_to_dense(voxel_model.data, voxel_model.dims)\n",
        "    else:\n",
        "        dense_voxel_data = voxel_model.data\n",
        "\n",
        "    fp.write('#binvox 1\\n')\n",
        "    fp.write('dim '+' '.join(map(str, voxel_model.dims))+'\\n')\n",
        "    fp.write('translate '+' '.join(map(str, voxel_model.translate))+'\\n')\n",
        "    fp.write('scale '+str(voxel_model.scale)+'\\n')\n",
        "    fp.write('data\\n')\n",
        "    if not voxel_model.axis_order in ('xzy', 'xyz'):\n",
        "        raise ValueError('Unsupported voxel model axis order')\n",
        "\n",
        "    if voxel_model.axis_order=='xzy':\n",
        "        voxels_flat = dense_voxel_data.flatten()\n",
        "    elif voxel_model.axis_order=='xyz':\n",
        "        voxels_flat = np.transpose(dense_voxel_data, (0, 2, 1)).flatten()\n",
        "\n",
        "    # keep a sort of state machine for writing run length encoding\n",
        "    state = voxels_flat[0]\n",
        "    ctr = 0\n",
        "    for c in voxels_flat:\n",
        "        if c==state:\n",
        "            ctr += 1\n",
        "            # if ctr hits max, dump\n",
        "            if ctr==255:\n",
        "                fp.write(chr(state))\n",
        "                fp.write(chr(ctr))\n",
        "                ctr = 0\n",
        "        else:\n",
        "            # if switch state, dump\n",
        "            fp.write(chr(state))\n",
        "            fp.write(chr(ctr))\n",
        "            state = c\n",
        "            ctr = 1\n",
        "    # flush out remainders\n",
        "    if ctr > 0:\n",
        "        fp.write(chr(state))\n",
        "        fp.write(chr(ctr))\n",
        "\n",
        "\n",
        "    import doctest\n",
        "    doctest.testmod()"
      ],
      "metadata": {
        "id": "xvZtMP2rGwgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample points from the solid model and surface model"
      ],
      "metadata": {
        "id": "U4ZFs0JzG8lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming point_cloud is a PyTorch tensor of shape [num_points, 3]\n",
        "# Example: point_cloud = torch.tensor([[1, 2, 3], [4, 5, 6], ...])\n",
        "def sample_from_grid_space(point_cloud):\n",
        "  # Initialize an empty grid\n",
        "  grid_size = 128\n",
        "  grid = torch.zeros(grid_size, grid_size, grid_size, dtype=torch.bool)\n",
        "\n",
        "  # Mark points inside the point cloud\n",
        "  # Convert 3D coordinates to grid indices\n",
        "  point_cloud_indices = point_cloud.long()\n",
        "  grid[point_cloud_indices[:, 0], point_cloud_indices[:, 1], point_cloud_indices[:, 2]] = 1\n",
        "\n",
        "  # Identify points outside the point cloud and sample\n",
        "  # Find indices where the grid is 0 (i.e., outside the point cloud)\n",
        "  outside_indices = torch.nonzero(grid == 0, as_tuple=True)\n",
        "\n",
        "  # Convert indices to 3D coordinates\n",
        "  outside_points = torch.stack(outside_indices, dim=-1)\n",
        "\n",
        "  # Optionally, randomly select a subset of points\n",
        "  desired_sample_size = 1024 # Example: desired sample size\n",
        "  if outside_points.shape[0] > desired_sample_size:\n",
        "      sample_indices = torch.randperm(outside_points.shape[0])[:desired_sample_size]\n",
        "      sample = outside_points[sample_indices]\n",
        "  else:\n",
        "      sample = outside_points\n",
        "  return sample"
      ],
      "metadata": {
        "id": "FN83An5NHQQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def normaldefinition_3D_real(void_data, k):\n",
        "    # void_data is expected to be a PyTorch tensor of shape [16, 1024, 3]\n",
        "    # Reshape void_data to [16*1024, 3] for distance calculation\n",
        "    void_data_reshaped = void_data.view(-1, 3)\n",
        "    dist = torch.cdist(void_data_reshaped, void_data_reshaped)\n",
        "    closest = dist.argsort(dim=1)\n",
        "\n",
        "    total_pts = void_data_reshaped.size(0)\n",
        "    planes = torch.zeros((total_pts, 6))\n",
        "\n",
        "    for i in range(total_pts):\n",
        "        # Adjust the indices to work with the reshaped data\n",
        "        normal_vect, xmn, ymn, zmn, knn_pt_coord = tangentplane_3D_real(closest[i, :k], void_data_reshaped, k)\n",
        "        planes[i, 0:3] = normal_vect\n",
        "        planes[i, 3:6] = torch.tensor([xmn, ymn, zmn])\n",
        "\n",
        "    planes_consist = normalconsistency_3D_real(planes)\n",
        "\n",
        "    # Reshape planes back to the original shape [batch, 1024, 6]\n",
        "    # planes_consist = planes_consist.view(void_data.shape[0], 1024, 6)\n",
        "    # Calculation used when no batch is needed\n",
        "    planes_consist = planes_consist.view(1024, 6)\n",
        "\n",
        "    return planes_consist, planes_consist\n",
        "\n",
        "def tangentplane_3D_real(closest_pt, ellipsoid_data, k):\n",
        "    knn_pt_id = closest_pt[:k]\n",
        "    knn_pt_coord = ellipsoid_data[knn_pt_id]\n",
        "\n",
        "    xmn = knn_pt_coord[:, 0].mean()\n",
        "    ymn = knn_pt_coord[:, 1].mean()\n",
        "    zmn = knn_pt_coord[:, 2].mean()\n",
        "\n",
        "    c = knn_pt_coord - torch.tensor([xmn, ymn, zmn]).to(device)\n",
        "\n",
        "    cov = torch.mm(c.t(), c) / k\n",
        "    u, s, vh = torch.svd(cov)\n",
        "    minevindex = s.argmin()\n",
        "    normal_vect = u[:, minevindex]\n",
        "\n",
        "    return normal_vect, xmn, ymn, zmn, knn_pt_coord\n",
        "\n",
        "def normalconsistency_3D_real(planes):\n",
        "    nbnormals = planes.size(0)\n",
        "    planes_consist = torch.zeros((nbnormals, 6))\n",
        "    planes_consist[:, 3:6] = planes[:, 3:6]\n",
        "\n",
        "    sensorcentre = torch.tensor([0, 0, 0])\n",
        "\n",
        "    for i in range(nbnormals):\n",
        "        p1 = (sensorcentre - planes[i, 3:6]) / torch.norm(sensorcentre - planes[i, 3:6])\n",
        "        p2 = planes[i, 0:3]\n",
        "\n",
        "        angle = torch.atan2(torch.norm(torch.cross(p1, p2)), torch.dot(p1, p2))\n",
        "\n",
        "        if (-np.pi/2 <= angle <= np.pi/2):\n",
        "            planes_consist[i, 0:3] = -planes[i, 0:3]\n",
        "        else:\n",
        "            planes_consist[i, 0:3] = planes[i, 0:3]\n",
        "\n",
        "    return planes_consist\n"
      ],
      "metadata": {
        "id": "Ig_oBLXWIH1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Farthest point sampling for augmented points\n",
        "# X+ = {x_i + n_i * eps}\n",
        "# X- = {x_i - n_i * eps}\n",
        "path = \"./binvoxSurfaceModels\"\n",
        "from pytorch3d.ops import sample_farthest_points\n",
        "files = os.listdir(path)\n",
        "arr = []\n",
        "max = 0\n",
        "eps = 1.0\n",
        "for i in range (len(files)):\n",
        "  item = f'./{path}/{files[i]}'\n",
        "  with open(item, 'rb') as f:\n",
        "    pcItem = read_as_coord_array(f)\n",
        "    data = torch.from_numpy(pcItem.data.astype(float)).to(device)\n",
        "    subData = data.permute(1,0).unsqueeze(0).to(device)\n",
        "    length = torch.full((1,), subData.shape[1]).to(device)\n",
        "    val = sample_farthest_points(subData, length, 2048)\n",
        "    normal = normaldefinition_3D_real(subData, 32)\n",
        "    normals = normal[1][:,:3]\n",
        "    S = subData.shape[1]\n",
        "    x_plus = torch.tensor((S, 3))\n",
        "    x_plus = subData + (eps * normals).to(device)\n",
        "    x_minus = torch.tensor((S, 3))\n",
        "    x_minus = subData - (eps * normals).to(device)\n",
        "    augmentedPoints = torch.cat((x_plus, x_minus), dim=0).to(device)\n",
        "    dataName = files[i].split(\".\")[0]\n",
        "    torch.save(augmentedPoints, f'./augmentedSurfacePoints/{dataName}.pt')\n",
        "    print(i, \" : \", dataName)"
      ],
      "metadata": {
        "id": "Y4ir8jZTHys8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r \"./augmentedSurfacePoints.zip\" \"./augmentedSurfacePoints\"\n",
        "! cp augmentedSurfacePoints.zip /save/path/augmentedSurfacePoints.zip"
      ],
      "metadata": {
        "id": "WpYb-yRPIhZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Farthest point sampling for the surface of a model\n",
        "path = \"./binvoxSurfaceModels\"\n",
        "from pytorch3d.ops import sample_farthest_points\n",
        "files = os.listdir(path)\n",
        "arr = []\n",
        "max = 0\n",
        "eps = 1.0\n",
        "for i in range (len(files)):\n",
        "  item = f'./{path}/{files[i]}'\n",
        "  with open(item, 'rb') as f:\n",
        "    pcItem = read_as_coord_array(f)\n",
        "    data = torch.from_numpy(pcItem.data.astype(float)).to(device)\n",
        "    subData = data.permute(1,0).unsqueeze(0).to(device)\n",
        "    length = torch.full((1,), subData.shape[1]).to(device)\n",
        "    surfacePoints = sample_farthest_points(subData, length, 2048)\n",
        "    dataName = files[i].split(\".\")[0]\n",
        "    torch.save(surfacePoints, f'./sampledSurfacePoints/{dataName}.pt')\n",
        "    print(i, \" : \", dataName)"
      ],
      "metadata": {
        "id": "BEmC5KcSHVNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r \"./sampledSurfacePoints.zip\" \"./sampledSurfacePoints\"\n",
        "! cp sampledSurfacePoints.zip /save/path/sampledSurfacePoints.zip"
      ],
      "metadata": {
        "id": "mJcdG6ZhIdDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Farthest point sampling for a solid model\n",
        "# Includes half points of the model and half points in the empty space outside\n",
        "path = \"./binvoxModels\"\n",
        "from pytorch3d.ops import sample_farthest_points\n",
        "files = os.listdir(path)\n",
        "arr = []\n",
        "max = 0\n",
        "eps = 1.0\n",
        "for i in range (len(files)):\n",
        "  item = f'./{path}/{files[i]}'\n",
        "  with open(item, 'rb') as f:\n",
        "    pcItem = read_as_coord_array(f)\n",
        "    data = torch.from_numpy(pcItem.data.astype(float)).to(device)\n",
        "    subData = data.permute(1,0).unsqueeze(0).to(device)\n",
        "    length = torch.full((1,), subData.shape[1]).to(device)\n",
        "    val = sample_farthest_points(subData, length, 1024)\n",
        "    empty_points = sample_from_grid_space(subData).to(device)\n",
        "    occupancyPoints = torch.cat((val[0].squeeze(0), empty_points)).to(device)\n",
        "    dataName = files[i].split(\".\")[0]\n",
        "    torch.save(occupancyPoints, f'./occupanyPoints/{dataName}.pt')\n",
        "    print(i, \" : \", dataName)"
      ],
      "metadata": {
        "id": "AQ-6WzzWHbJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r \"./occupancyKernelPoints.zip\" \"./occupancyKernelPoints\"\n",
        "! cp occupancyKernelPoints.zip /save/path/occupancyKernelPoints.zip"
      ],
      "metadata": {
        "id": "KsdeNncTIZdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "def plotNormalsAndPoints(tensor):\n",
        "    # Assuming tensor is of shape [points, 6] where the first 3 columns are normal vectors and the last 3 are points\n",
        "    # Normalize the vectors to ensure they are of unit length\n",
        "    normalized_tensor = tensor[:, :3]\n",
        "\n",
        "    # Separate the XYZ components of the points\n",
        "    points_x = tensor[:, 3]\n",
        "    points_y = tensor[:, 4]\n",
        "    points_z = tensor[:, 5]\n",
        "\n",
        "    # Create a scatter plot for the points\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Add the points to the plot\n",
        "    fig.add_trace(go.Scatter3d(x=points_x, y=points_y, z=points_z, mode='markers', marker=dict(size=2, color='blue')))\n",
        "\n",
        "    # Add the normal vectors as arrows\n",
        "    for i in range(len(points_x)):\n",
        "        fig.add_trace(go.Cone(x=[points_x[i]], y=[points_y[i]], z=[points_z[i]], u=[normalized_tensor[i, 0]], v=[normalized_tensor[i, 1]], w=[normalized_tensor[i, 2]], sizemode=\"absolute\", sizeref=1.0, anchor=\"tail\"))\n",
        "\n",
        "    # Set the layout\n",
        "    fig.update_layout(scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z'))\n",
        "\n",
        "    # Show the plot\n",
        "    fig.show()\n"
      ],
      "metadata": {
        "id": "PaLcMP0LImqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "def plotPointClouds(input):\n",
        "  threshold = 0.5\n",
        "  input = input.permute(1,0)\n",
        "  input = input.cpu()\n",
        "  x = input[0]\n",
        "  y = input[1]\n",
        "  z = input[2]\n",
        "\n",
        "  fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z, mode=\"markers\", marker=dict(size=1))])\n",
        "\n",
        "  fig.update_layout(scene=dict(xaxis_title='X', yaxis_title=\"Y\", zaxis_title=\"Z\"))\n",
        "\n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "MXQ51TyVIs9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def map_points_to_grid(points, grid_size, num_features, points_features):\n",
        "    \"\"\"\n",
        "    Map points to a grid and perform average pooling of features within each grid cell.\n",
        "\n",
        "    Args:\n",
        "    - points: Tensor of shape (B, N, 3) containing (x, y, z) coordinates of points, where B is the batch size, N is the number of points per batch.\n",
        "              Also takes an input tensor of shape (B, N, num_features) containing features for each point.\n",
        "              Point values range from 0-128\n",
        "    - grid_size: Tuple containing (grid_x, grid_y, grid_z) dimensions of the grid.\n",
        "    - num_features: Number of features per point.\n",
        "\n",
        "    Returns:\n",
        "    - grid_tensor: Tensor of shape (B, 8, 8, 8, num_features) containing the average pooled features within each grid cell.\n",
        "    \"\"\"\n",
        "\n",
        "    B, N, _ = points.size()\n",
        "    grid_tensor = torch.zeros(B, 8, 8, 8, num_features).to(device)\n",
        "\n",
        "    # Calculate the size of each grid cell\n",
        "    cell_size = (128 / grid_size[0], 128 / grid_size[1], 128 / grid_size[2])\n",
        "\n",
        "    # Iterate through each batch\n",
        "    for batch_idx in range(B):\n",
        "        # Create a tensor to accumulate features for each grid cell\n",
        "        cell_features = torch.zeros(8, 8, 8, num_features).to(device)\n",
        "        count = torch.zeros(8, 8, 8).to(device)\n",
        "\n",
        "        # Iterate through each point in the batch\n",
        "        for point, features in zip(points[batch_idx], points_features[batch_idx]):\n",
        "            x, y, z = point.tolist()\n",
        "\n",
        "            # Map point to grid cell\n",
        "            grid_x = min(int(x / cell_size[0]), grid_size[0] - 1)\n",
        "            grid_y = min(int(y / cell_size[1]), grid_size[1] - 1)\n",
        "            grid_z = min(int(z / cell_size[2]), grid_size[2] - 1)\n",
        "\n",
        "            # Accumulate features for the corresponding grid cell\n",
        "            cell_features[grid_x, grid_y, grid_z] += features\n",
        "            count[grid_x, grid_y, grid_z] += 1\n",
        "\n",
        "        # Avoid division by zero\n",
        "        count[count == 0] = 1\n",
        "\n",
        "        # Perform average pooling\n",
        "        grid_tensor[batch_idx] = cell_features / count.unsqueeze(-1)\n",
        "\n",
        "    return grid_tensor\n",
        "\n",
        "# Define grid size\n",
        "grid_size = (8, 8, 8)\n",
        "\n",
        "# Map points to the grid and perform average pooling\n",
        "# grid_tensor = map_points_to_grid(inputTensor, grid_size, out.size(2), out)\n",
        "# grid_tensor = grid_tensor.permute(0,4,1,2,3)\n",
        "# print(\"Grid tensor shape:\", grid_tensor.shape)"
      ],
      "metadata": {
        "id": "997whk4GJkMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "C4BJ3cRLI7_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on Convolutional Occupancy Networks: https://github.com/autonomousvision/convolutional_occupancy_networks/tree/master"
      ],
      "metadata": {
        "id": "iKDG6057JPDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetBlock(nn.Module):\n",
        "  def __init__(self, in_size, out_size, h_size=None):\n",
        "    super(ResNetBlock, self).__init__()\n",
        "\n",
        "    if h_size is None:\n",
        "      h_size = min(in_size, out_size)\n",
        "\n",
        "    self.in_size = in_size\n",
        "    self.h_size = h_size\n",
        "    self.out_size = out_size\n",
        "\n",
        "    self.fc_0 = nn.Linear(in_size, h_size)\n",
        "    self.fc_1 = nn.Linear(h_size, out_size)\n",
        "    self.actfn = nn.ReLU()\n",
        "\n",
        "    if in_size == out_size:\n",
        "      self.shortcut = None\n",
        "    else:\n",
        "      self.shortcut = nn.Linear(in_size, out_size, bias=False)\n",
        "\n",
        "    nn.init.zeros_(self.fc_1.weight)\n",
        "\n",
        "  def forward(self, x):\n",
        "    net = self.fc_0(self.actfn(x))\n",
        "    dx = self.fc_1(self.actfn(net))\n",
        "\n",
        "    if self.shortcut is not None:\n",
        "      x_s = self.shortcut(x)\n",
        "    else:\n",
        "      x_s = x\n",
        "\n",
        "    return x_s + dx"
      ],
      "metadata": {
        "id": "uLbI_XyzI96X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_3d_coordinate(p, padding=0.1):\n",
        "  p_nor = p / (1 + padding + 10e-4) # (-0.5, 0.5)\n",
        "  p_nor = p_nor + 0.5 # range (0, 1)\n",
        "  # f there are outliers out of the range\n",
        "  if p_nor.max() >= 1:\n",
        "      p_nor[p_nor >= 1] = 1 - 10e-4\n",
        "  if p_nor.min() < 0:\n",
        "      p_nor[p_nor < 0] = 0.0\n",
        "  return p_nor"
      ],
      "metadata": {
        "id": "hZlPn2rfI_2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def coordinate2index(x, reso):\n",
        "  x = (x * reso).long()\n",
        "  index = x[:,:, 0] + reso * (x[:,:,1] + reso * x[:,:,2])\n",
        "  index = index[:, None, :]\n",
        "  index = index % reso**3\n",
        "  return index"
      ],
      "metadata": {
        "id": "sxPcTkBEJG3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PointNetEncoder(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim, hidden_dim=128, num_blocks=5, grid_resolution=None):\n",
        "   super(PointNetEncoder, self).__init__()\n",
        "\n",
        "   self.fc_1 = nn.Linear(in_dim, 2*hidden_dim)\n",
        "   self.blocks = nn.ModuleList([\n",
        "       ResNetBlock(2*hidden_dim, hidden_dim) for i in range(num_blocks)\n",
        "   ])\n",
        "   self.fc_2 = nn.Linear(hidden_dim, out_dim)\n",
        "   self.actfn = nn.ReLU()\n",
        "   self.hidden_dim = hidden_dim\n",
        "   self.scatter = scatter_max\n",
        "   self.reso_grid = grid_resolution\n",
        "\n",
        "\n",
        "  def pool_local(self, xy, index, c):\n",
        "    bs, fea_dim = c.size(0), c.size(2)\n",
        "    c_out = 0\n",
        "    fea = self.scatter(c.permute(0,2,1), index['grid'], dim_size=self.reso_grid**3)\n",
        "    if(self.scatter == scatter_max):\n",
        "      fea = fea[0]\n",
        "    fea = fea.gather(dim=2, index=index['grid'].expand(-1, fea_dim, -1))\n",
        "    c_out += fea\n",
        "    return c_out.permute(0, 2, 1)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    b_size = x.size(0)\n",
        "\n",
        "    coord = {}\n",
        "    index = {}\n",
        "    coord['grid'] = normalize_3d_coordinate(x.clone())\n",
        "    index['grid'] = coordinate2index(x, self.reso_grid)\n",
        "\n",
        "    net = self.fc_1(x)\n",
        "\n",
        "    net = self.blocks[0](net)\n",
        "    for block in self.blocks[1:]:\n",
        "      pooled = self.pool_local(coord, index, net)\n",
        "      net = torch.cat([net, pooled], dim=2)\n",
        "      net = block(net)\n",
        "\n",
        "    x = self.fc_2(net)\n",
        "    return x"
      ],
      "metadata": {
        "id": "9rmy37PGJY9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PointEncoder = PointNetEncoder(3, 32, grid_resolution=8).to(device)"
      ],
      "metadata": {
        "id": "4BsdJt4LJgap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UNet"
      ],
      "metadata": {
        "id": "2UBpskjjJppC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3d(in_channels, out_channels, kernel_size, bias, padding=1):\n",
        "  return nn.Conv3d(in_channels, out_channels, kernel_size, padding=padding, bias=bias)"
      ],
      "metadata": {
        "id": "rzbyTzdqJrqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_conv(in_channels, out_channels, kernel_size, order, num_groups, padding=1):\n",
        "  modules = []\n",
        "\n",
        "  for i, char in enumerate(order):\n",
        "      if char == 'r':\n",
        "          modules.append(('ReLU', nn.ReLU(inplace=True)))\n",
        "      elif char == 'l':\n",
        "          modules.append(('LeakyReLU', nn.LeakyReLU(negative_slope=0.1, inplace=True)))\n",
        "      elif char == 'e':\n",
        "          modules.append(('ELU', nn.ELU(inplace=True)))\n",
        "      elif char == 'c':\n",
        "          # add learnable bias only in the absence of batchnorm/groupnorm\n",
        "          bias = not ('g' in order or 'b' in order)\n",
        "          modules.append(('conv', conv3d(in_channels, out_channels, kernel_size, bias, padding=padding)))\n",
        "      elif char == 'g':\n",
        "          is_before_conv = i < order.index('c')\n",
        "          if is_before_conv:\n",
        "              num_channels = in_channels\n",
        "          else:\n",
        "              num_channels = out_channels\n",
        "\n",
        "          # use only one group if the given number of groups is greater than the number of channels\n",
        "          if num_channels < num_groups:\n",
        "              num_groups = 1\n",
        "\n",
        "          assert num_channels % num_groups == 0, f'Expected number of channels in input to be divisible by num_groups. num_channels={num_channels}, num_groups={num_groups}'\n",
        "          modules.append(('groupnorm', nn.GroupNorm(num_groups=num_groups, num_channels=num_channels)))\n",
        "      elif char == 'b':\n",
        "          is_before_conv = i < order.index('c')\n",
        "          if is_before_conv:\n",
        "              modules.append(('batchnorm', nn.BatchNorm3d(in_channels)))\n",
        "          else:\n",
        "              modules.append(('batchnorm', nn.BatchNorm3d(out_channels)))\n",
        "  return modules"
      ],
      "metadata": {
        "id": "dqG4G6zXJtod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleConv(nn.Sequential):\n",
        "  def __init__(self, in_c, out_c, kernel_size=3, order='crg', num_groups=8, padding=1):\n",
        "    super(SingleConv, self).__init__()\n",
        "\n",
        "    for name, module in create_conv(in_c, out_c, kernel_size, order, num_groups, padding=padding):\n",
        "      self.add_module(name, module)"
      ],
      "metadata": {
        "id": "QIhtTQ0vJvMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Sequential):\n",
        "  def __init__(self, in_c, out_c, encoder, kernel_size=3, order='crg', num_groups=8):\n",
        "    super(DoubleConv, self).__init__()\n",
        "    if encoder:\n",
        "      conv1_in_channels = in_c\n",
        "      conv1_out_channels = out_c // 2\n",
        "      if conv1_out_channels < in_c:\n",
        "        conv1_out_channels = in_c\n",
        "      conv2_in_channels, conv2_out_channels = conv1_out_channels, out_c\n",
        "    else:\n",
        "      conv1_in_channels, conv1_out_channels = in_c, out_c\n",
        "      conv2_in_channels, conv2_out_channels = out_c, out_c\n",
        "\n",
        "    self.add_module('SingleConv1',\n",
        "                    SingleConv(conv1_in_channels, conv1_out_channels, kernel_size, order, num_groups))\n",
        "    self.add_module('SingleConv2',\n",
        "                    SingleConv(conv2_in_channels, conv2_out_channels, kernel_size, order, num_groups))"
      ],
      "metadata": {
        "id": "7WEKbynWJxT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, in_c, out_c, conv_kernel_size=3, apply_pooling=True, pool_kernel_size=(2,2,2), pool_type=\"max\", basic_module=DoubleConv, conv_layer_order='crg', num_groups=8):\n",
        "    super(Encoder, self).__init__()\n",
        "    if apply_pooling:\n",
        "      self.pooling = nn.MaxPool3d(kernel_size=pool_kernel_size)\n",
        "    else:\n",
        "      self.pooling = None\n",
        "    self.basic_module = basic_module(in_c, out_c, encoder=True, kernel_size=conv_kernel_size, order=conv_layer_order, num_groups=num_groups)\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.pooling is not None:\n",
        "      x = self.pooling(x)\n",
        "    x = self.basic_module(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "PFIyufhZJygB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Upsampling(nn.Module):\n",
        "  def __init__(self, transposed_conv, in_c=None, out_c=None, kernel_size=3, scale_factor=(2,2,2), mode='nearest'):\n",
        "    super(Upsampling, self).__init__()\n",
        "\n",
        "    if transposed_conv:\n",
        "      self.upsample = nn.ConvTranspose3d(in_c, out_c, kernel_size=kernel_size, stride=scale_factor, padding=1)\n",
        "    else:\n",
        "      self.upsample = partial(self._interpolate, mode=mode)\n",
        "\n",
        "  def forward(self, encoder_features, x):\n",
        "    output_size = encoder_features.size()[2:]\n",
        "    return self.upsample(x, output_size)\n",
        "\n",
        "  @staticmethod\n",
        "  def _interpolate(x, size, mode):\n",
        "    return F.interpolate(x, size=size, mode=mode)"
      ],
      "metadata": {
        "id": "qj7fTJSAJz8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, in_c, out_c, kernel_size=3, scale_factor=(2,2,2), basic_module=DoubleConv, conv_layer_order='crg', num_groups=8, mode='nearest', transposed_conv=False):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.upsampling = Upsampling(transposed_conv=transposed_conv, in_c=in_c, out_c=out_c, kernel_size=kernel_size, scale_factor=scale_factor, mode=mode)\n",
        "\n",
        "    self.joining = partial(self._joining, concat=True)\n",
        "\n",
        "    self.basic_module = basic_module(in_c, out_c,\n",
        "                                         encoder=False,\n",
        "                                         kernel_size=kernel_size,\n",
        "                                         order=conv_layer_order,\n",
        "                                         num_groups=num_groups)\n",
        "\n",
        "  def forward(self, encoder_features, x):\n",
        "    x = self.upsampling(encoder_features=encoder_features, x=x)\n",
        "    x = self.joining(encoder_features, x)\n",
        "    x = self.basic_module(x)\n",
        "    return x\n",
        "\n",
        "  @staticmethod\n",
        "  def _joining(encoder_features, x, concat):\n",
        "    if concat:\n",
        "      return torch.cat((encoder_features, x), dim=1)\n",
        "    else:\n",
        "      return encoder_features + x"
      ],
      "metadata": {
        "id": "r6Q9gNYyJ1QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def number_of_features_per_level(init_channel_number, num_levels):\n",
        "    return [init_channel_number * 2 ** k for k in range(num_levels)]"
      ],
      "metadata": {
        "id": "SsyVUv-YJ5Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Abstract3DUNet(nn.Module):\n",
        "  def __init__(self, in_c, out_c, final_sigmoid, basic_module, f_maps=128, layer_order='gcr', num_groups=8, num_levels=3, is_segmentation=False, testing=False, **kwargs):\n",
        "    super(Abstract3DUNet, self).__init__()\n",
        "\n",
        "    self.testing = testing\n",
        "\n",
        "    if isinstance(f_maps, int):\n",
        "      f_maps = number_of_features_per_level(f_maps, num_levels=num_levels)\n",
        "\n",
        "    encoders = []\n",
        "    for i, out_feature_num in enumerate(f_maps):\n",
        "      if i == 0:\n",
        "        encoder = Encoder(in_c, out_feature_num, apply_pooling=False, basic_module=basic_module, conv_layer_order=layer_order, num_groups=num_groups)\n",
        "      else:\n",
        "        encoder = Encoder(f_maps[i - 1], out_feature_num, basic_module=basic_module,\n",
        "                                  conv_layer_order=layer_order, num_groups=num_groups)\n",
        "      encoders.append(encoder)\n",
        "    self.encoders = nn.ModuleList(encoders)\n",
        "\n",
        "    decoders = []\n",
        "    reversed_f_maps = list(reversed(f_maps))\n",
        "    for i in range(len(reversed_f_maps)-1):\n",
        "        if basic_module == DoubleConv:\n",
        "            in_feature_num = reversed_f_maps[i] + reversed_f_maps[i + 1]\n",
        "        else:\n",
        "            in_feature_num = reversed_f_maps[i]\n",
        "\n",
        "        out_feature_num = reversed_f_maps[i + 1]\n",
        "        decoder = Decoder(in_feature_num, out_feature_num, basic_module=basic_module,\n",
        "                          conv_layer_order=layer_order, num_groups=num_groups)\n",
        "        decoders.append(decoder)\n",
        "    self.decoders = nn.ModuleList(decoders)\n",
        "\n",
        "    self.final_conv = nn.Conv3d(f_maps[0], out_c, 1)\n",
        "    self.final_activation=None\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoders_features = []\n",
        "    for encoder in self.encoders:\n",
        "        encoders_features.insert(0, x)\n",
        "        x = encoder(x)\n",
        "        # reverse the encoder outputs to be aligned with the decoder\n",
        "\n",
        "    encoders_features = encoders_features[0:]\n",
        "\n",
        "    # decoder part\n",
        "    for decoder, encoder_features in zip(self.decoders, encoders_features):\n",
        "        # pass the output from the corresponding encoder and the output\n",
        "        # of the previous decoder\n",
        "        x = decoder(encoder_features, x)\n",
        "\n",
        "    x = self.final_conv(x)\n",
        "    return x, encoders_features"
      ],
      "metadata": {
        "id": "jY4iRrtfJ6st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet3D(Abstract3DUNet):\n",
        "  def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=32, layer_order='gcr',\n",
        "              num_groups=8, num_levels=3, is_segmentation=False, **kwargs):\n",
        "    super(UNet3D, self).__init__(in_c=in_channels, out_c=out_channels, final_sigmoid=final_sigmoid,\n",
        "                                  basic_module=DoubleConv, f_maps=f_maps, layer_order=layer_order,\n",
        "                                  num_groups=num_groups, num_levels=num_levels, is_segmentation=is_segmentation,\n",
        "                                  **kwargs)"
      ],
      "metadata": {
        "id": "FiSclCNmKCWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Unet = UNet3D(32, 32).to(device)"
      ],
      "metadata": {
        "id": "uoGeR7_XKD58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "BHvmGAeoKLSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, PointEncoder, Unet):\n",
        "    super(Model, self).__init__()\n",
        "\n",
        "    self.pointEncoder = PointEncoder\n",
        "    self.Unet = Unet\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoded_points = self.pointEncoder(x)\n",
        "    grid_tensor = map_points_to_grid(x, grid_size, encoded_points.size(2), encoded_points)\n",
        "    grid_tensor = grid_tensor.permute(0,4,1,2,3)\n",
        "    feature_grid, ef = self.Unet(grid_tensor)\n",
        "    expanded_grid = nn.functional.interpolate(feature_grid.detach(), scale_factor=16, mode='trilinear')\n",
        "    return expanded_grid"
      ],
      "metadata": {
        "id": "Ky2A26piKFk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(PointEncoder, Unet).to(device)"
      ],
      "metadata": {
        "id": "BRmPlf6uKH3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "asNKePsfKJTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training helper functions"
      ],
      "metadata": {
        "id": "5TlgQiEtKNDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def createGramMatrix(points_i, points_j):\n",
        "  \"\"\"\n",
        "    Input: A set of points_i and points_j [batch_size, num_points, num_values] Ex: [16, 2048, 3] and [16, 5000, 3]\n",
        "    Output: A matrix of size [batch_size, num_points_i, num_points_j, 2, 3]\n",
        "            Which represents a set of values at each ij index\n",
        "  \"\"\"\n",
        "  jdx = torch.arange(points_j.shape[1]).unsqueeze(0).repeat(points_i.shape[1], 1)\n",
        "  idx = torch.arange(points_i.shape[1]).unsqueeze(1).repeat(1, points_j.shape[1])\n",
        "\n",
        "  pairs = torch.cat((points_i[:, idx, :], points_j[:, jdx, :]), dim=3).reshape(points_i.shape[0], points_i.shape[1], points_j.shape[1], 2, -1)\n",
        "  return pairs"
      ],
      "metadata": {
        "id": "8vcAY1NhKO2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trilinear_interpolation(query_points, grid):\n",
        "    # Extract the coordinates of the eight surrounding vertices\n",
        "    grid = grid.permute(0,2,3,4,1)\n",
        "    query_points_floor = query_points.floor().long() - 2\n",
        "    x0, y0, z0 = query_points_floor[:,:,0], query_points_floor[:,:,1], query_points_floor[:,:,2]\n",
        "    x1, y1, z1 = x0 + 1, y0 + 1, z0 + 1\n",
        "\n",
        "    batch_enum = torch.arange(query_points.shape[0]).unsqueeze(1)\n",
        "\n",
        "    # Extract the values at the eight surrounding vertices\n",
        "    c000 = grid[batch_enum, x0, y0, z0]\n",
        "    c001 = grid[batch_enum, x0, y0, z1]\n",
        "    c010 = grid[batch_enum, x0, y1, z0]\n",
        "    c011 = grid[batch_enum, x0, y1, z1]\n",
        "    c100 = grid[batch_enum, x1, y0, z0]\n",
        "    c101 = grid[batch_enum, x1, y0, z1]\n",
        "    c110 = grid[batch_enum, x1, y1, z0]\n",
        "    c111 = grid[batch_enum, x1, y1, z1]\n",
        "\n",
        "    # Compute the interpolation weights and add 1s to match the last dimension of c000 ... c111\n",
        "    u = (query_points[:,:,0] - x0.float()).unsqueeze(-1).expand(query_points.shape[0],query_points.shape[1], grid.shape[4])\n",
        "    v = (query_points[:,:,1] - y0.float()).unsqueeze(-1).expand(query_points.shape[0],query_points.shape[1], grid.shape[4])\n",
        "    w = (query_points[:,:,2] - z0.float()).unsqueeze(-1).expand(query_points.shape[0],query_points.shape[1], grid.shape[4])\n",
        "\n",
        "    # Perform trilinear interpolation\n",
        "    interpolated_value = (1 - u) * (1 - v) * (1 - w) * c000 + \\\n",
        "                         (1 - u) * (1 - v) * w * c001 + \\\n",
        "                         (1 - u) * v * (1 - w) * c010 + \\\n",
        "                         (1 - u) * v * w * c011 + \\\n",
        "                         u * (1 - v) * (1 - w) * c100 + \\\n",
        "                         u * (1 - v) * w * c101 + \\\n",
        "                         u * v * (1 - w) * c110 + \\\n",
        "                         u * v * w * c111\n",
        "    return interpolated_value"
      ],
      "metadata": {
        "id": "6LStBt3gKTsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculateTheta(x_tilde, x_tilde_prime):\n",
        "  norm = torch.linalg.norm(x_tilde, dim=-1).unsqueeze(3)\n",
        "  norm_prime = torch.linalg.norm(x_tilde_prime, dim=-1).unsqueeze(3)\n",
        "  numerator = torch.linalg.norm(norm_prime * x_tilde - norm * x_tilde_prime, dim=-1)\n",
        "  denominator = torch.linalg.norm(norm_prime * x_tilde + norm * x_tilde_prime, dim=-1)\n",
        "  theta = torch.atan2(numerator, denominator)\n",
        "  return theta"
      ],
      "metadata": {
        "id": "dLambpNGKZpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculateNeuralSpline(x):\n",
        "    x_tilde = x[..., 0]  # Extract x_tilde\n",
        "    x_tilde_prime = x[..., 1]  # Extract x_tilde_prime\n",
        "\n",
        "    theta = calculateTheta(x_tilde, x_tilde_prime)\n",
        "    firstTerm = (torch.linalg.norm(x_tilde, dim=-1) * torch.linalg.norm(x_tilde_prime, dim=-1) / np.pi)\n",
        "    secondTerm = (torch.sin(theta) + 2 * (np.pi - theta) * torch.cos(theta))\n",
        "    kernelVal = firstTerm * secondTerm\n",
        "    return kernelVal.squeeze()  # Remove singleton dimensions"
      ],
      "metadata": {
        "id": "T6iTqLL_KdNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculateKernel(points_i, points_j, grid):\n",
        "  # Get features for each point by trilinearly interpolating from output grid\n",
        "  features_i = trilinear_interpolation(points_i, grid)\n",
        "  features_j = trilinear_interpolation(points_j, grid)\n",
        "  # Concat features with points\n",
        "  concat_points_i = torch.cat((points_i, features_i), dim=2)\n",
        "  concat_points_j = torch.cat((points_j, features_j), dim=2)\n",
        "  # Calculate gram matrix\n",
        "  matrix = createGramMatrix(concat_points_i, concat_points_j)\n",
        "  # Pass matrix into calculateNeuralSpline\n",
        "  Kns = calculateNeuralSpline(matrix)\n",
        "  # Return values\n",
        "  return Kns"
      ],
      "metadata": {
        "id": "Xvh4goPmKg9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f_x(alpha, new_points, original_points, grid):\n",
        "  return alpha * calculateKernel(new_points, original_points, grid)"
      ],
      "metadata": {
        "id": "TGo6Nj_4KiG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ],
      "metadata": {
        "id": "mXluHvjKKkHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "step = 0\n",
        "train_loader_len = len(train_loader)\n",
        "yVector = torch.ones((b_size, 2048, 1)).to(device)\n",
        "yVector[0][1024:] = -1\n",
        "yVolume = torch.ones((b_size, 2048, 1)).to(device)\n",
        "yVolume[0][1024:] = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(f'***************** epoch: {epoch} *****************')\n",
        "  for idx, data in enumerate(train_loader):\n",
        "    if(idx >= train_loader_len - 1):\n",
        "      continue\n",
        "    step += 1\n",
        "    tempPointsList = []\n",
        "    tempOccupancyPointsList = []\n",
        "    tempSurfacePointsList = []\n",
        "    idx+=1\n",
        "    for item in range(len(data)):\n",
        "      augmentedPoints = torch.load(f'./augmentedSurfacePoints/{data[item]}')\n",
        "      tempPointsList.append(augmentedPoints)\n",
        "      occupancyPoints = torch.load(f'./occupancyPoints_1/{data[item]}')\n",
        "      tempOccupancyPointsList.append(occupancyPoints)\n",
        "      surfacePoints = torch.load(f'./sampledSurfacePoints/{data[item]}')\n",
        "      tempSurfacePointsList.append(surfacePoints[0])\n",
        "    inputPoints = torch.stack(tempPointsList, dim=0).to(device)\n",
        "    occupancyTensor = torch.stack(tempOccupancyPointsList, dim=0).to(device)\n",
        "    surfaceTensor = torch.stack(tempSurfacePointsList, dim=0).to(device)\n",
        "\n",
        "    expanded_grid = model(inputPoints)\n",
        "    points_i = inputPoints.clone()\n",
        "    points_j = inputPoints.clone()\n",
        "    features_i = trilinear_interpolation(points_i, expanded_grid)\n",
        "    features_j = trilinear_interpolation(points_j, expanded_grid)\n",
        "    concat_points_i = torch.cat((points_i, features_i), dim=2)\n",
        "    concat_points_j = torch.cat((points_j, features_j), dim=2)\n",
        "    matrix = createGramMatrix(concat_points_i, concat_points_j).detach()\n",
        "    Kns = calculateNeuralSpline(matrix)\n",
        "\n",
        "    lambdaVal = 0.0001\n",
        "    identityMatrix = torch.zeros(Kns.shape[0], Kns.shape[1], Kns.shape[2]).to(device)\n",
        "    identityMatrix[:, torch.arange(Kns.shape[1]), torch.arange(Kns.shape[2])] = 1\n",
        "\n",
        "    kernelMatrix = torch.linalg.inv((Kns + (lambdaVal * identityMatrix)))\n",
        "    alpha = torch.matmul(kernelMatrix, yVector).to(device)\n",
        "\n",
        "    occupancyPred = f_x(alpha, occupancyTensor, inputPoints, expanded_grid)\n",
        "    surfacePred = f_x(alpha, surfaceTensor, inputPoints, expanded_grid)\n",
        "    bceLoss = nn.BCEWithLogitsLoss()\n",
        "    occupancyLoss = bceLoss(occupancyPred, yVolume)\n",
        "    l1_lambda = 0.001\n",
        "    surfaceLoss = l1_lambda * torch.sum(torch.abs(surfacePred), dim=1)\n",
        "    loss = occupancyLoss + surfaceLoss\n",
        "    print(\"Loss\", loss.detach().item())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "59pZId35KlhJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}